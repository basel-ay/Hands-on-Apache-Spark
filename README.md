# Hands-on-Apache-Spark

Apache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute data processing tasks across multiple computers, either on its own or in tandem with other distributed computing tools. These two qualities are key to the worlds of big data and machine learning, which require the marshaling of massive computing power to crunch through large data stores. Spark also takes some of the programming burdens of these tasks off the shoulders of developers with an easy-to-use API that abstracts away much of the grunt work of distributed computing and big data processing.

![image](https://github.com/basel-ay/Hands-on-Apache-Spark/assets/64821137/f6bbb336-3615-4f5e-bc1e-675113f798a9)

## Features of Apache Spark

Apache Spark has following features:

* Speed − Spark helps to run an application in Hadoop cluster, up to 100 times faster in memory, and 10 times faster when running on disk. This is possible by reducing number of read/write operations to disk. It stores the intermediate processing data in memory.

* Supports multiple languages − Spark provides built-in APIs in Java, Scala, or Python. Therefore, you can write applications in different languages. Spark comes up with 80 high-level operators for interactive querying.

* Advanced Analytics − Spark not only supports ‘Map’ and ‘reduce’. It also supports SQL queries, Streaming data, Machine learning (ML), and Graph algorithms.

## Components of Spark
The following illustration depicts the different components of Spark:

<p align="center">
  <img src="https://github.com/basel-ay/Hands-on-Apache-Spark/assets/64821137/81991047-3cb8-4b4d-a182-41d124ec4870" />
</p>
